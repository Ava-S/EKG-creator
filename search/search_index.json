{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PromG Library","text":"<p><code>PromG</code> collects queries for modeling, importing, enriching and analyzing event data as Event Knowledge Graphs (EKGs). The queries are run against a Neo4j instance. </p> <p>All scripts and queries are licensed under LGPL v3.0, see LICENSE. Copyright information is provided within each Project.</p> <p>This site contains the project documentation for <code>PromG</code>.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ol> <li>Tutorials</li> <li>How-To Guides</li> <li>Modules<ol> <li>Database Manager</li> </ol> </li> <li> <p>Explanation</p> </li> <li> <p>PromG Documentation: index.md tutorials.md</p> </li> <li>How-To Guides: how-to-guides.md</li> <li>Modules:<ul> <li></li> </ul> </li> <li>explanation.md</li> </ol> <p>Quickly find what you're looking for depending on your use case by looking at the different pages.</p>"},{"location":"#get-started","title":"Get started","text":""},{"location":"#promg","title":"PromG","text":"<p>The library can be installed in Pyhton using pip <code>pip install promg</code>.</p>"},{"location":"#neo4j","title":"Neo4j","text":"<p>The library assumes that Neo4j is installed.</p> <p>Install Neo4j:</p> <ul> <li>Use the Neo4j Desktop  (recommended), or</li> <li>Neo4j Community Server</li> </ul>"},{"location":"#create-a-new-graph-database","title":"Create a new graph database","text":"<ul> <li>The scripts in this release assume password \"12345678\".</li> <li>The scripts assume the server to be available at the default URL <code>bolt://localhost:7687</code></li> <li>You can modify this also in the script.</li> <li>ensure to allocate enough memory to your database, advised: <code>dbms.memory.heap.max_size=5G</code></li> <li>the script expects the <code>Neo4j APOC library</code> to be installed as a plugin, see https://neo4j.com/labs/apoc/</li> </ul> <p>For example projects that use this library, have a look at EKG BPI Challenges, EKG Inferring missing identifiers and EKG for AutoTwin EU GA n. 101092021.</p>"},{"location":"#projectsmodules","title":"Projects/Modules","text":"<p>The following projects are part of this repository</p>"},{"location":"#event-knowledge-graphs","title":"Event Knowledge Graphs","text":"<p>We use Event Knowledge Graphs as basis for our data model as they already naturally model Events, Activities, Entities and their relations for Process Mining. The EKGs are stored in a labeled property graph in Neo4J.</p>"},{"location":"#publications","title":"Publications","text":"<ul> <li>Stefan Esser, Dirk Fahland: Multi-Dimensional Event Data in Graph   Databases. CoRR abs/2005.14552, Journal on Data Semantics, DOI: 10.1007/s13740-021-00122-1 (   2020)</li> <li>Esser, Stefan. (2020, February 19). A Schema Framework for Graph Event Data. Master thesis. Eindhoven University of   Technology. https://doi.org/10.5281/zenodo.3820037</li> </ul>"},{"location":"#oced-pg","title":"OCED-PG","text":"<p>We developed a reference implementation for Object-Centric Event Data OCED. OCED-PG is a declarative extract-load-transformT framework, that maps the raw data to a corresponding EKG, using the semantic header as a basis.</p> <p>We proposed a three-layer approach to create a semantic-aware representation and storage system for OCED. </p> <ul> <li> <p>Base ontology: The OCED proposal is formalized as a PG-schema [1] providing a common interface for process querying. The schema defines a base ontology for representing and transforming OCED, which includes a semantic layer (defining the OCED concepts) and a record layer (defining concepts for generic data records from a legacy system and how they are related to the semantic layer). </p> </li> <li> <p>Reference ontology: The base ontology is specialized into a domain-specific reference ontology using PG-schema's inheritance mechanism. The reference ontology includes a semantic layer (defining the domain's semantic objects, events, and relations), and a record layer (defining in which legacy records the domain-level concepts are stored). The structural definitions are extended with rules to transform data in the record layer into nodes and relationships of the semantic layer, similar to ontology-based data access.</p> </li> <li>OCED-PG: declarative extract-load-transform (ELT) framework. OCED-PG load the legacy data records into the graph DB as a record layer. We then transform the data records into OCED by automatically translating the transformation rules of step (2) into queries over the record layer.</li> </ul>"},{"location":"#multi-process-discovery-and-analysis","title":"Multi-process Discovery and Analysis","text":""},{"location":"#publications_1","title":"Publications","text":""},{"location":"#task-identification","title":"Task Identification","text":""},{"location":"#publications_2","title":"Publications","text":""},{"location":"#custom-modules","title":"Custom Modules","text":""},{"location":"#missing-case-identifiers-inference","title":"Missing Case Identifiers Inference","text":"<p>Method to infer missing case identifiers in event data by exploiting knowledge about the activities and their locations.</p>"},{"location":"#publications_3","title":"Publications","text":""},{"location":"#scripts","title":"Scripts","text":""},{"location":"#main-script","title":"Main script","text":"<p>There is one script (orchestrator) that is used by applications to create an Event Knowledge graph. This script makes use of the library.</p>"},{"location":"#data_managers","title":"Data_managers","text":"<ul> <li>data_managers/datastructures.py --&gt; transforms the JSON file describing the different datasets into a class + additional methods</li> <li>data_managers/semantic_header.py --&gt; transforms the JSON file describing the semantic header into a class + additional methods</li> <li>data_managers/interpreters.py --&gt; Class that contains information about in what query language the semantic header and data structures should be interpreter</li> </ul>"},{"location":"#database_managers","title":"Database_managers","text":"<ul> <li>database_managers/authentication.py  --&gt; class containing the credentials to create connection to database. Local credentials are includes. In case you want to create a remote connection, add the following piece of code to a (gitignored) file.</li> </ul> <pre><code>remote = Credentials(\n    uri=\"[your_uri]\",\n    user=\"neo4j\",\n    password=\"[your_password]\"\n)\n</code></pre> <ul> <li>database_managers/db_connection.py --&gt; class responsible for making the connection to the database and to communicate with the database</li> <li>database_managers/EventKnowledgeGraph.py --&gt; class responsible for making (changes to) the EKG and to request data from the EKG. Makes use of several modules.</li> </ul>"},{"location":"#ekg_modules","title":"EKG_Modules","text":"<ul> <li>ekg_modules/db_management.py --&gt; general module to manage the database</li> <li>ekg_modules/data_importer.py --&gt; imports the data stored in the records into the EKG</li> <li>ekg_modules/ekg_builder_semantic_header.py --&gt; creates the required nodes and relations as specified in the semantic header</li> <li>ekg_modules/inference_engine.py --&gt; module responsible for inferring missing information</li> <li>ekg_modules/ekg_analysis.py --&gt; module for analysis of the EKG (e.g. create process model)</li> <li>ekg_modules/ekg_custom_module.py --&gt; module to create custom queries, specific for this example</li> </ul>"},{"location":"#cypherqueries","title":"CypherQueries","text":"<p>Contains repeatable pieces of Cypher Queries for all necessary parts. - cypher_queries/query_translators --&gt; translate semantic header and data structures into Cypher - cypher_queries/query_library --&gt; contains all cypher queries for the EKG modules - cypher_queries/custom_query_library --&gt; contains all custom cypher queries for this example for the EKG modules</p>"},{"location":"data-importer/","title":"Module: Database Manager","text":""},{"location":"data-importer/#promg.modules.data_importer.Importer","title":"<code>Importer</code>","text":"<p>Create Importer module</p> <p>Imports data using the dataset description files</p> <p>Parameters:</p> Name Type Description Default <code>data_structures</code> <code>DatasetDescriptions</code> <p>DatasetDescriptions object describing the different datasets</p> required <code>use_sample</code> <code>bool</code> <p>boolean indicating whether a sample can be used</p> <code>False</code> <code>use_preprocessed_files</code> <code>bool</code> <p>boolean indicating that preprocessed files can be used</p> <code>False</code> <p>Examples:</p> <p>Example without sample and preprocessed files</p> <pre><code>&gt;&gt;&gt; from promg.modules.data_importer import Importer\n&gt;&gt;&gt; # set dataset name\n&gt;&gt;&gt; dataset_name = 'BPIC17'\n&gt;&gt;&gt; # location of json file with dataset_description\n&gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n&gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n&gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions)\nThe module to import data is returned.\nThe module won't use a sample, nor the preprocessed files\n</code></pre> <p>Example with sample and preprocessed files</p> <pre><code>&gt;&gt;&gt; from promg.modules.data_importer import Importer\n&gt;&gt;&gt; # set dataset name\n&gt;&gt;&gt; dataset_name = 'BPIC17'\n&gt;&gt;&gt; # location of json file with dataset_description\n&gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n&gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n&gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions,\n&gt;&gt;&gt;                     use_sample = True,\n&gt;&gt;&gt;                     use_preprocessed_files = True)\nThe module to import data is returned.\nThe module will use the sample and the preprocessed files\nif they exist, in case they do not exist, they are created\n</code></pre> Source code in <code>promg\\modules\\data_importer.py</code> <pre><code>class Importer:\n\"\"\"\n        Create Importer module\n\n        Imports data using the dataset description files\n\n        Args:\n            data_structures: DatasetDescriptions object describing the different datasets\n            use_sample: boolean indicating whether a sample can be used\n            use_preprocessed_files: boolean indicating that preprocessed files can be used\n\n        Examples:\n            Example without sample and preprocessed files\n            &gt;&gt;&gt; from promg.modules.data_importer import Importer\n            &gt;&gt;&gt; # set dataset name\n            &gt;&gt;&gt; dataset_name = 'BPIC17'\n            &gt;&gt;&gt; # location of json file with dataset_description\n            &gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n            &gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n            &gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions)\n            The module to import data is returned.\n            The module won't use a sample, nor the preprocessed files\n\n            Example with sample and preprocessed files\n\n            &gt;&gt;&gt; from promg.modules.data_importer import Importer\n            &gt;&gt;&gt; # set dataset name\n            &gt;&gt;&gt; dataset_name = 'BPIC17'\n            &gt;&gt;&gt; # location of json file with dataset_description\n            &gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n            &gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n            &gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions,\n            &gt;&gt;&gt;                     use_sample = True,\n            &gt;&gt;&gt;                     use_preprocessed_files = True)\n            The module to import data is returned.\n            The module will use the sample and the preprocessed files\n            if they exist, in case they do not exist, they are created\n    \"\"\"\n\n    def __init__(self, data_structures: DatasetDescriptions,\n                 use_sample: bool = False, use_preprocessed_files: bool = False):\n        self.connection = DatabaseConnection()\n        self.structures = data_structures.structures\n        self.records = SemanticHeader().records\n\n        self.load_batch_size = 20000\n        self.use_sample = use_sample\n        self.use_preprocessed_files = use_preprocessed_files\n        self.load_status = 0\n\n    def _update_load_status(self):\n\"\"\"\n        Method to keep track of the load status of the different record nodes\n        \"\"\"\n        self.connection.exec_query(di_ql.get_update_load_status_query,\n                                   **{\n                                       \"current_load_status\": self.load_status\n                                   })\n        self.load_status += 1\n\n    def import_data(self) -&gt; None:\n\"\"\"\n        Method that imports the data records into the graph database as (:Record) nodes.\n        The records contain the attributes as described in the dataset descriptions.\n        Method also adds the specific record labels as specified by the semantic header.\n\n        Examples:\n            &gt;&gt;&gt; importer.import_data()\n            The records of the dataset described in the dataset descriptions are imported as (:Record) nodes with\n            appropriate attributes and labels\n        \"\"\"\n        for structure in self.structures:\n            record_constructors = self._get_record_constructors_by_labels(structure=structure)\n            file_directory = structure.file_directory\n            # read in all file names that match this structure\n            for file_name in structure.file_names:\n                # read and import the nodes\n                df_log = structure.read_data_set(file_directory, file_name, use_sample=self.use_sample,\n                                                 use_preprocessed_file=self.use_preprocessed_files)\n                df_log[\"loadStatus\"] = self.load_status\n                self._import_nodes_from_data(df_log=df_log, file_name=file_name,\n                                             record_constructors=record_constructors)\n\n            if structure.has_datetime_attribute():\n                # once all events are imported, we convert the string timestamp to the timestamp as used in Cypher\n                self._reformat_timestamps(structure=structure)\n                self._update_load_status()\n\n            self._filter_nodes(structure=structure)  # filter nodes according to the structure\n\n            self._remove_load_status_attribute()  # removes temporary properties\n\n    @Performance.track(\"structure\")\n    def _reformat_timestamps(self, structure: DataStructure):\n\"\"\"\n        Method that converts the timestamps of (:Record) nodes imported according to a specific structure\n        from string to DateTime or Time attribute.\n\n        Args:\n            structure: The data structure that contains the just imported nodes\n        \"\"\"\n\n        datetime_formats = structure.get_datetime_formats()\n        for attribute, datetime_format in datetime_formats.items():\n            if datetime_format.is_epoch:\n                self.connection.exec_query(di_ql.get_convert_epoch_to_timestamp_query,\n                                           **{\n                                               \"attribute\": attribute,\n                                               \"datetime_object\": datetime_format\n                                           })\n\n            self.connection.exec_query(di_ql.get_make_timestamp_date_query,\n                                       **{\n                                           \"attribute\": attribute,\n                                           \"datetime_object\": datetime_format,\n                                           \"load_status\": self.load_status\n                                       })\n\n    @Performance.track(\"structure\")\n    def _filter_nodes(self, structure):\n        # TODO: check function and\n        for boolean in (True, False):\n            attribute_values_pairs_filtered = structure.get_attribute_value_pairs_filtered(exclude=boolean)\n            for name, values in attribute_values_pairs_filtered.items():\n                self.connection.exec_query(di_ql.get_filter_events_by_property_query,\n                                           **{\n                                               \"prop\": name, \"values\": values, \"exclude\": boolean,\n                                               \"load_status\": self.load_status\n                                           })\n\n    @Performance.track(\"structure\")\n    def _remove_load_status_attribute(self):\n\"\"\"\n        Method that removes the load status attribute from the (:Record) nodes\n        \"\"\"\n        self.connection.exec_query(di_ql.get_finalize_import_events_query)\n\n    @Performance.track(\"file_name\")\n    def _import_nodes_from_data(self, df_log: DataFrame, file_name: str,\n                                record_constructors: List[Dict[str, Union[RecordConstructor, bool]]]):\n\"\"\"\n        Method that imports records from a dataframe log as (:Record) nodes and assigns the correct labels\n\n        Args:\n            df_log: The records to be imported in Dataframe format\n            file_name: The file name from which the records to be imported originate from\n            record_constructors: A list indicating which record labels should be (possibly) assigned to the imported\n            (:Record) nodes\n        \"\"\"\n\n        # start with batch 0 and increment until everything is imported\n        batch = 0\n        print(\"\\n\")\n        pbar = tqdm(total=math.ceil(len(df_log) / self.load_batch_size), position=0)\n\n        labels_constructor = di_ql.get_label_constructors(record_constructors)\n\n        while batch * self.load_batch_size &lt; len(df_log):\n            pbar.set_description(f\"Loading data from {file_name} from batch {batch}\")\n\n            # import the events in batches, use the records of the log\n            batch_without_nans = [{k: int(v) if isinstance(v, np.integer) else v for k, v in m.items()\n                                   if (isinstance(v, list) and len(v) &gt; 0) or (not pd.isna(v) and v is not None)}\n                                  for m in\n                                  df_log[batch * self.load_batch_size:(batch + 1) * self.load_batch_size].to_dict(\n                                      orient='records')]\n\n            self.connection.exec_query(di_ql.get_create_nodes_by_importing_batch_query,\n                                       **{\n                                           \"batch\": batch_without_nans,\n                                           \"labels_constructors\": labels_constructor\n                                       })\n\n            pbar.update(1)\n            batch += 1\n        pbar.close()\n\n    def _get_record_constructors_by_labels(self, structure: DataStructure) -&gt; List[\n        Dict[str, Union[RecordConstructor, bool]]]:\n\"\"\"\n        Method to determine for the structure which record labels are optional/required for the (:Record) nodes\n\n\n        Args:\n            structure: The DataStructure for which the labels are checked\n        \"\"\"\n\n        constructors = []\n        labels = structure.labels\n\n        # loop over all record constructions\n        for record_constructor in self.records:\n            # if labels are defined, determine the intersection between the labels of structure and the labels of the\n            # record construction\n            if labels is not None:\n                intersection = list(set(labels) &amp; set(record_constructor.record_labels))\n            else:  # take the record labels of the record constructor\n                intersection = record_constructor.record_labels\n            if len(intersection) &gt; 0:  # label of record constructor is in intersection or labels is not defined\n                required = True\n                # if there is a where condition defined, then imported (:Record) nodes might not have the record label\n                # hence required = false\n                if record_constructor.prevalent_record.where_condition != \"\":\n                    required = False\n                else:\n                    # check whether each required attribute in the record constructor is also required according to\n                    # the structure (except index, is always present)\n                    for required_attribute in record_constructor.required_attributes:\n                        if required_attribute == \"index\":\n                            continue\n                        if required_attribute not in structure.attributes or structure.attributes[\n                            required_attribute].optional:\n                            required = False\n                constructors.append({\"required\": required, \"record_constructor\": record_constructor})\n        return constructors\n</code></pre>"},{"location":"data-importer/#promg.modules.data_importer.Importer.import_data","title":"<code>import_data()</code>","text":"<p>Method that imports the data records into the graph database as (:Record) nodes. The records contain the attributes as described in the dataset descriptions. Method also adds the specific record labels as specified by the semantic header.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; importer.import_data()\nThe records of the dataset described in the dataset descriptions are imported as (:Record) nodes with\nappropriate attributes and labels\n</code></pre> Source code in <code>promg\\modules\\data_importer.py</code> <pre><code>def import_data(self) -&gt; None:\n\"\"\"\n    Method that imports the data records into the graph database as (:Record) nodes.\n    The records contain the attributes as described in the dataset descriptions.\n    Method also adds the specific record labels as specified by the semantic header.\n\n    Examples:\n        &gt;&gt;&gt; importer.import_data()\n        The records of the dataset described in the dataset descriptions are imported as (:Record) nodes with\n        appropriate attributes and labels\n    \"\"\"\n    for structure in self.structures:\n        record_constructors = self._get_record_constructors_by_labels(structure=structure)\n        file_directory = structure.file_directory\n        # read in all file names that match this structure\n        for file_name in structure.file_names:\n            # read and import the nodes\n            df_log = structure.read_data_set(file_directory, file_name, use_sample=self.use_sample,\n                                             use_preprocessed_file=self.use_preprocessed_files)\n            df_log[\"loadStatus\"] = self.load_status\n            self._import_nodes_from_data(df_log=df_log, file_name=file_name,\n                                         record_constructors=record_constructors)\n\n        if structure.has_datetime_attribute():\n            # once all events are imported, we convert the string timestamp to the timestamp as used in Cypher\n            self._reformat_timestamps(structure=structure)\n            self._update_load_status()\n\n        self._filter_nodes(structure=structure)  # filter nodes according to the structure\n\n        self._remove_load_status_attribute()  # removes temporary properties\n</code></pre>"},{"location":"db-management/","title":"Module: Database Manager","text":""},{"location":"db-management/#promg.modules.db_management.DBManagement","title":"<code>DBManagement</code>","text":"<p>Create DBManagement module</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.db_management import DBManagement\n&gt;&gt;&gt; db_manager = DBManagement()\n</code></pre> Source code in <code>promg\\modules\\db_management.py</code> <pre><code>class DBManagement:\n\"\"\"\n        Create DBManagement module\n        Examples:\n            &gt;&gt;&gt; from promg.modules.db_management import DBManagement\n            &gt;&gt;&gt; db_manager = DBManagement()\n    \"\"\"\n\n    def __init__(self):\n        self.connection = DatabaseConnection()\n        self.db_name = self.connection.db_name\n\n    @Performance.track()\n    def clear_db(self, replace: bool = True) -&gt; None:\n\"\"\"\n        Replace or clear the entire database by a new one.\n\n        Note:\n            Note about difference between replacing and clearing.\n\n            - Replace: results in an Empty database that is completely replaced with a new database.\n                - Replacing a database is faster than clearing a database.\n                - Only possible when\n                    - you have an enterprise license\n                    - you are running a local instance on the free desktop version\n\n            - Clear: Results in an empty database, however constraints are still in place.\n                - Clearing a database takes longer\n                - Independent of license\n\n        Args:\n            replace: boolean to indicate whether the database may be replaced\n\n        Examples:\n            &gt;&gt;&gt; db_manager.clear(replace=True)\n            Results in an Empty database that is completely replaced with a new database\n                (i.e. constraints are also removed)\n\n            &gt;&gt;&gt; db_manager.clear(replace=False)\n            Results in an empty database, however constraints are still in place.\n            Clearing an entire database takes longer.\n        \"\"\"\n        if replace:\n            self.connection.exec_query(dbm_ql.get_replace_db_query, **{\"db_name\": self.db_name})\n        else:\n            self.connection.exec_query(dbm_ql.get_delete_relationships_query)\n            self.connection.exec_query(dbm_ql.get_delete_nodes_query)\n\n    @Performance.track()\n    def set_constraints(self) -&gt; None:\n\"\"\"\n        Set constraints in Neo4j instance.\n\n        - sysId property is used as index for (:Entity) nodes\n\n        Examples:\n            &gt;&gt;&gt; db_managers.set_constraints()\n            sysId is used as index for (:Entity) nodes\n        \"\"\"\n        # # for implementation only (not required by schema or patterns)\n        # self.connection.exec_query(dbm_ql.get_constraint_unique_event_id_query)\n        #\n        # required by core pattern\n        # self.connection.exec_query(dbm_ql.get_constraint_unique_entity_uid_query)\n        #\n        # self.connection.exec_query(dbm_ql.get_constraint_unique_log_id_query)\n\n        self.connection.exec_query(dbm_ql.get_set_sysid_index_query)\n\n    def get_all_rel_types(self) -&gt; List[str]:\n\"\"\"\n        Get all relationship types that are present in Neo4j instance\n\n        Returns:\n            A list of strings with all relationship types present in the Neo4j instance\n        \"\"\"\n\n        # execute the query and store the result\n        result = self.connection.exec_query(dbm_ql.get_all_rel_types_query)\n        # in case there are no rel types, the result is None\n        # return in this case an emtpy list\n        if result is None:\n            return []\n        # store the results in a list\n        result = [record[\"rel_type\"] for record in result]\n        return result\n\n    def get_all_node_labels(self) -&gt; Set[str]:\n\"\"\"\n        Get all node labels that are present in Neo4j instance\n\n        Returns:\n            A list of strings with all node labels present in the Neo4j instance\n        \"\"\"\n\n        # execute the query and store the result\n        result = self.connection.exec_query(dbm_ql.get_all_node_labels_query)\n        # in case there are no labels, return an empty set\n        if result is None:\n            return set([])\n        # some nodes have multiple labels, which are returned as a list of labels\n        # therefore we need to flatten the result and take the set\n        result = set([record for sublist in result for record in sublist[\"label\"]])\n        return result\n\n    def get_statistics(self) -&gt; List[Dict[str, any]]:\n\"\"\"\n        Get the count of nodes per label and the count of relationships per type\n\n        Returns:\n            A list containing dictionaries with the label/relationship and its count\n        \"\"\"\n\n        def make_empty_list_if_none(_list: Optional[List[Dict[str, str]]]):\n            if _list is not None:\n                return _list\n            else:\n                return []\n\n        node_count = self.connection.exec_query(dbm_ql.get_node_count_query)\n        edge_count = self.connection.exec_query(dbm_ql.get_edge_count_query)\n        agg_edge_count = self.connection.exec_query(dbm_ql.get_aggregated_edge_count_query)\n        result = \\\n            make_empty_list_if_none(node_count) + \\\n            make_empty_list_if_none(edge_count) + \\\n            make_empty_list_if_none(agg_edge_count)\n        return result\n\n    def print_statistics(self) -&gt; None:\n\"\"\"\n        Print the statistics nicely using tabulate\n        \"\"\"\n        print(tabulate(self.get_statistics()))\n</code></pre>"},{"location":"db-management/#promg.modules.db_management.DBManagement.clear_db","title":"<code>clear_db(replace=True)</code>","text":"<p>Replace or clear the entire database by a new one.</p> Note <p>Note about difference between replacing and clearing.</p> <ul> <li> <p>Replace: results in an Empty database that is completely replaced with a new database.</p> <ul> <li>Replacing a database is faster than clearing a database.</li> <li>Only possible when<ul> <li>you have an enterprise license</li> <li>you are running a local instance on the free desktop version</li> </ul> </li> </ul> </li> <li> <p>Clear: Results in an empty database, however constraints are still in place.</p> <ul> <li>Clearing a database takes longer</li> <li>Independent of license</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>replace</code> <code>bool</code> <p>boolean to indicate whether the database may be replaced</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; db_manager.clear(replace=True)\nResults in an Empty database that is completely replaced with a new database\n    (i.e. constraints are also removed)\n</code></pre> <pre><code>&gt;&gt;&gt; db_manager.clear(replace=False)\nResults in an empty database, however constraints are still in place.\nClearing an entire database takes longer.\n</code></pre> Source code in <code>promg\\modules\\db_management.py</code> <pre><code>@Performance.track()\ndef clear_db(self, replace: bool = True) -&gt; None:\n\"\"\"\n    Replace or clear the entire database by a new one.\n\n    Note:\n        Note about difference between replacing and clearing.\n\n        - Replace: results in an Empty database that is completely replaced with a new database.\n            - Replacing a database is faster than clearing a database.\n            - Only possible when\n                - you have an enterprise license\n                - you are running a local instance on the free desktop version\n\n        - Clear: Results in an empty database, however constraints are still in place.\n            - Clearing a database takes longer\n            - Independent of license\n\n    Args:\n        replace: boolean to indicate whether the database may be replaced\n\n    Examples:\n        &gt;&gt;&gt; db_manager.clear(replace=True)\n        Results in an Empty database that is completely replaced with a new database\n            (i.e. constraints are also removed)\n\n        &gt;&gt;&gt; db_manager.clear(replace=False)\n        Results in an empty database, however constraints are still in place.\n        Clearing an entire database takes longer.\n    \"\"\"\n    if replace:\n        self.connection.exec_query(dbm_ql.get_replace_db_query, **{\"db_name\": self.db_name})\n    else:\n        self.connection.exec_query(dbm_ql.get_delete_relationships_query)\n        self.connection.exec_query(dbm_ql.get_delete_nodes_query)\n</code></pre>"},{"location":"db-management/#promg.modules.db_management.DBManagement.set_constraints","title":"<code>set_constraints()</code>","text":"<p>Set constraints in Neo4j instance.</p> <ul> <li>sysId property is used as index for (:Entity) nodes</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; db_managers.set_constraints()\nsysId is used as index for (:Entity) nodes\n</code></pre> Source code in <code>promg\\modules\\db_management.py</code> <pre><code>@Performance.track()\ndef set_constraints(self) -&gt; None:\n\"\"\"\n    Set constraints in Neo4j instance.\n\n    - sysId property is used as index for (:Entity) nodes\n\n    Examples:\n        &gt;&gt;&gt; db_managers.set_constraints()\n        sysId is used as index for (:Entity) nodes\n    \"\"\"\n    # # for implementation only (not required by schema or patterns)\n    # self.connection.exec_query(dbm_ql.get_constraint_unique_event_id_query)\n    #\n    # required by core pattern\n    # self.connection.exec_query(dbm_ql.get_constraint_unique_entity_uid_query)\n    #\n    # self.connection.exec_query(dbm_ql.get_constraint_unique_log_id_query)\n\n    self.connection.exec_query(dbm_ql.get_set_sysid_index_query)\n</code></pre>"},{"location":"db-management/#promg.modules.db_management.DBManagement.get_all_rel_types","title":"<code>get_all_rel_types()</code>","text":"<p>Get all relationship types that are present in Neo4j instance</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings with all relationship types present in the Neo4j instance</p> Source code in <code>promg\\modules\\db_management.py</code> <pre><code>def get_all_rel_types(self) -&gt; List[str]:\n\"\"\"\n    Get all relationship types that are present in Neo4j instance\n\n    Returns:\n        A list of strings with all relationship types present in the Neo4j instance\n    \"\"\"\n\n    # execute the query and store the result\n    result = self.connection.exec_query(dbm_ql.get_all_rel_types_query)\n    # in case there are no rel types, the result is None\n    # return in this case an emtpy list\n    if result is None:\n        return []\n    # store the results in a list\n    result = [record[\"rel_type\"] for record in result]\n    return result\n</code></pre>"},{"location":"db-management/#promg.modules.db_management.DBManagement.get_all_node_labels","title":"<code>get_all_node_labels()</code>","text":"<p>Get all node labels that are present in Neo4j instance</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>A list of strings with all node labels present in the Neo4j instance</p> Source code in <code>promg\\modules\\db_management.py</code> <pre><code>def get_all_node_labels(self) -&gt; Set[str]:\n\"\"\"\n    Get all node labels that are present in Neo4j instance\n\n    Returns:\n        A list of strings with all node labels present in the Neo4j instance\n    \"\"\"\n\n    # execute the query and store the result\n    result = self.connection.exec_query(dbm_ql.get_all_node_labels_query)\n    # in case there are no labels, return an empty set\n    if result is None:\n        return set([])\n    # some nodes have multiple labels, which are returned as a list of labels\n    # therefore we need to flatten the result and take the set\n    result = set([record for sublist in result for record in sublist[\"label\"]])\n    return result\n</code></pre>"},{"location":"db-management/#promg.modules.db_management.DBManagement.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Get the count of nodes per label and the count of relationships per type</p> <p>Returns:</p> Type Description <code>List[Dict[str, any]]</code> <p>A list containing dictionaries with the label/relationship and its count</p> Source code in <code>promg\\modules\\db_management.py</code> <pre><code>def get_statistics(self) -&gt; List[Dict[str, any]]:\n\"\"\"\n    Get the count of nodes per label and the count of relationships per type\n\n    Returns:\n        A list containing dictionaries with the label/relationship and its count\n    \"\"\"\n\n    def make_empty_list_if_none(_list: Optional[List[Dict[str, str]]]):\n        if _list is not None:\n            return _list\n        else:\n            return []\n\n    node_count = self.connection.exec_query(dbm_ql.get_node_count_query)\n    edge_count = self.connection.exec_query(dbm_ql.get_edge_count_query)\n    agg_edge_count = self.connection.exec_query(dbm_ql.get_aggregated_edge_count_query)\n    result = \\\n        make_empty_list_if_none(node_count) + \\\n        make_empty_list_if_none(edge_count) + \\\n        make_empty_list_if_none(agg_edge_count)\n    return result\n</code></pre>"},{"location":"db-management/#promg.modules.db_management.DBManagement.print_statistics","title":"<code>print_statistics()</code>","text":"<p>Print the statistics nicely using tabulate</p> Source code in <code>promg\\modules\\db_management.py</code> <pre><code>def print_statistics(self) -&gt; None:\n\"\"\"\n    Print the statistics nicely using tabulate\n    \"\"\"\n    print(tabulate(self.get_statistics()))\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>PromG</code> project code.</p>"},{"location":"reference/#core-modules","title":"Core Modules","text":""},{"location":"reference/#table-of-contents","title":"Table Of Contents","text":"<ol> <li>DatabaseManager</li> <li>How-To Guides</li> <li>Reference</li> <li>Explanation</li> </ol>"},{"location":"reference/#promg.database_managers.EventKnowledgeGraph","title":"<code>promg.database_managers.EventKnowledgeGraph</code>","text":""},{"location":"reference/#promg.modules.data_importer.Importer","title":"<code>promg.modules.data_importer.Importer</code>","text":"<p>Create Importer module</p> <p>Imports data using the dataset description files</p> <p>Parameters:</p> Name Type Description Default <code>data_structures</code> <code>DatasetDescriptions</code> <p>DatasetDescriptions object describing the different datasets</p> required <code>use_sample</code> <code>bool</code> <p>boolean indicating whether a sample can be used</p> <code>False</code> <code>use_preprocessed_files</code> <code>bool</code> <p>boolean indicating that preprocessed files can be used</p> <code>False</code> <p>Examples:</p> <p>Example without sample and preprocessed files</p> <pre><code>&gt;&gt;&gt; from promg.modules.data_importer import Importer\n&gt;&gt;&gt; # set dataset name\n&gt;&gt;&gt; dataset_name = 'BPIC17'\n&gt;&gt;&gt; # location of json file with dataset_description\n&gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n&gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n&gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions)\nThe module to import data is returned.\nThe module won't use a sample, nor the preprocessed files\n</code></pre> <p>Example with sample and preprocessed files</p> <pre><code>&gt;&gt;&gt; from promg.modules.data_importer import Importer\n&gt;&gt;&gt; # set dataset name\n&gt;&gt;&gt; dataset_name = 'BPIC17'\n&gt;&gt;&gt; # location of json file with dataset_description\n&gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n&gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n&gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions,\n&gt;&gt;&gt;                     use_sample = True,\n&gt;&gt;&gt;                     use_preprocessed_files = True)\nThe module to import data is returned.\nThe module will use the sample and the preprocessed files\nif they exist, in case they do not exist, they are created\n</code></pre>"},{"location":"reference/#promg.modules.data_importer.Importer.import_data","title":"<code>import_data()</code>","text":"<p>Method that imports the data records into the graph database as (:Record) nodes. The records contain the attributes as described in the dataset descriptions. Method also adds the specific record labels as specified by the semantic header.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; importer.import_data()\nThe records of the dataset described in the dataset descriptions are imported as (:Record) nodes with\nappropriate attributes and labels\n</code></pre>"}]}