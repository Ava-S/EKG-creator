{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PromG Library","text":"<p><code>PromG</code> collects queries for modeling, importing, enriching and analyzing event data as Event Knowledge Graphs (EKGs). The queries are run against a Neo4j instance. </p> <p>All scripts and queries are licensed under LGPL v3.0, see LICENSE. Copyright information is provided within each Project.</p> <p>This site contains the project documentation for <code>PromG</code>.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<p>Quickly find what you're looking for depending on your use case by looking at the different pages.</p> <ol> <li>Getting Started</li> <li>Tutorials</li> <li>How-To Guides</li> <li>Modules<ol> <li>Database Management Modules<ol> <li>Database Manager</li> </ol> </li> <li>Data Storage<ol> <li>Data Importer</li> <li>Transformer</li> <li>Exporter</li> </ol> </li> <li>Inference<ol> <li>Inference Engine</li> </ol> </li> <li>Analysis<ol> <li>Process Model Disovery</li> <li>Task Identification </li> </ol> </li> </ol> </li> <li>Related Publications</li> </ol> <p>For example projects that use this library, have a look at EKG BPI Challenges, EKG Inferring missing identifiers.</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#promg","title":"PromG","text":"<p>The library can be installed in Pyhton using pip <code>pip install promg</code>.</p>"},{"location":"getting-started/#neo4j","title":"Neo4j","text":"<p>The library assumes that Neo4j is installed.</p> <p>Install Neo4j:</p> <ul> <li>Use the Neo4j Desktop  (recommended), or</li> <li>Neo4j Community Server</li> </ul>"},{"location":"getting-started/#create-a-new-graph-database","title":"Create a new graph database","text":"<ul> <li>The scripts in this release assume password \"12345678\".</li> <li>The scripts assume the server to be available at the default URL <code>bolt://localhost:7687</code></li> <li>You can modify this also in the script.</li> <li>ensure to allocate enough memory to your database, advised: <code>dbms.memory.heap.max_size=5G</code></li> <li>the script expects the <code>Neo4j APOC library</code> to be installed as a plugin, see https://neo4j.com/labs/apoc/</li> </ul>"},{"location":"getting-started/#ensure-database-is-running","title":"Ensure database is running","text":"<ul> <li>When executing the main script, ensure that your database instance is running.</li> </ul>"},{"location":"module-0verview/","title":"Module 0verview","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>PromG</code> project code.</p>"},{"location":"module-0verview/#core-modules","title":"Core Modules","text":""},{"location":"module-0verview/#table-of-contents","title":"Table Of Contents","text":"<ol> <li>Database Management Modules<ol> <li>Database Manager</li> </ol> </li> <li>Data Storage<ol> <li>Data Importer</li> <li>Transformer</li> <li>Exporter</li> </ol> </li> <li>Inference<ol> <li>Inference Engine</li> </ol> </li> <li>Analysis<ol> <li>Process Model Disovery</li> <li>Task Identification </li> </ol> </li> </ol>"},{"location":"module-data-importer/","title":"Module: Database Manager","text":""},{"location":"module-data-importer/#promg.modules.data_importer.Importer","title":"<code>Importer</code>","text":"<p>Create Importer module</p> <p>Imports data using the dataset description files</p> <p>Parameters:</p> Name Type Description Default <code>data_structures</code> <code>DatasetDescriptions</code> <p>DatasetDescriptions object describing the different datasets</p> required <code>use_sample</code> <code>bool</code> <p>boolean indicating whether a sample can be used</p> <code>False</code> <code>use_preprocessed_files</code> <code>bool</code> <p>boolean indicating that preprocessed files can be used</p> <code>False</code> <p>Examples:</p> <p>Example without sample and preprocessed files</p> <pre><code>&gt;&gt;&gt; from promg.modules.data_importer import Importer\n&gt;&gt;&gt; # set dataset name\n&gt;&gt;&gt; dataset_name = 'BPIC17'\n&gt;&gt;&gt; # location of json file with dataset_description\n&gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n&gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n&gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions)\nThe module to import data is returned.\nThe module won't use a sample, nor the preprocessed files\n</code></pre> <p>Example with sample and preprocessed files</p> <pre><code>&gt;&gt;&gt; from promg.modules.data_importer import Importer\n&gt;&gt;&gt; # set dataset name\n&gt;&gt;&gt; dataset_name = 'BPIC17'\n&gt;&gt;&gt; # location of json file with dataset_description\n&gt;&gt;&gt; ds_path = Path(f'json_files/{dataset_name}_DS.json')\n&gt;&gt;&gt; dataset_descriptions = DatasetDescriptions(ds_path)\n&gt;&gt;&gt; importer = Importer(data_structures = dataset_descriptions,\n&gt;&gt;&gt;                     use_sample = True,\n&gt;&gt;&gt;                     use_preprocessed_files = True)\nThe module to import data is returned.\nThe module will use the sample and the preprocessed files\nif they exist, in case they do not exist, they are created\n</code></pre>"},{"location":"module-data-importer/#promg.modules.data_importer.Importer.import_data","title":"<code>import_data()</code>","text":"<p>Method that imports the data records into the graph database as (:Record) nodes. The records contain the attributes as described in the dataset descriptions. Method also adds the specific record labels as specified by the semantic header.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; importer.import_data()\nThe records of the dataset described in the dataset descriptions are imported as (:Record) nodes with\nappropriate attributes and labels\n</code></pre>"},{"location":"module-db-management/","title":"Module: Database Manager","text":""},{"location":"module-db-management/#promg.modules.db_management.DBManagement","title":"<code>DBManagement</code>","text":"<p>Create DBManagement module</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.db_management import DBManagement\n&gt;&gt;&gt; db_manager = DBManagement()\n</code></pre>"},{"location":"module-db-management/#promg.modules.db_management.DBManagement.clear_db","title":"<code>clear_db(replace=True)</code>","text":"<p>Replace or clear the entire database by a new one.</p> Note <p>Note about difference between replacing and clearing.</p> <ul> <li> <p>Replace: results in an Empty database that is completely replaced with a new database.</p> <ul> <li>Replacing a database is faster than clearing a database.</li> <li>Only possible when<ul> <li>you have an Neo4j enterprise license</li> <li>you are running a local instance on the free Neo4j desktop version</li> </ul> </li> </ul> </li> <li> <p>Clear: Results in an empty database, however constraints are still in place.</p> <ul> <li>Clearing a database takes longer</li> <li>Independent of license</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>replace</code> <code>bool</code> <p>boolean to indicate whether the database may be replaced</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; db_manager.clear(replace=True)\nResults in an Empty database that is completely replaced with a new database\n    (i.e. constraints are also removed)\n</code></pre> <pre><code>&gt;&gt;&gt; db_manager.clear(replace=False)\nResults in an empty database, however constraints are still in place.\nClearing an entire database takes longer.\n</code></pre>"},{"location":"module-db-management/#promg.modules.db_management.DBManagement.set_constraints","title":"<code>set_constraints()</code>","text":"<p>Set constraints in Neo4j instance.</p> <ul> <li>sysId property is used as index for (:Entity) nodes</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; db_managers.set_constraints()\nsysId is used as index for (:Entity) nodes\n</code></pre>"},{"location":"module-db-management/#promg.modules.db_management.DBManagement.get_all_rel_types","title":"<code>get_all_rel_types()</code>","text":"<p>Get all relationship types that are present in Neo4j instance</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings with all relationship types present in the Neo4j instance</p>"},{"location":"module-db-management/#promg.modules.db_management.DBManagement.get_all_node_labels","title":"<code>get_all_node_labels()</code>","text":"<p>Get all node labels that are present in Neo4j instance</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>A list of strings with all node labels present in the Neo4j instance</p>"},{"location":"module-db-management/#promg.modules.db_management.DBManagement.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Get the count of nodes per label and the count of relationships per type</p> <p>Returns:</p> Type Description <code>List[Dict[str, any]]</code> <p>A list containing dictionaries with the label/relationship and its count</p>"},{"location":"module-db-management/#promg.modules.db_management.DBManagement.print_statistics","title":"<code>print_statistics()</code>","text":"<p>Print the statistics nicely using tabulate</p>"},{"location":"module-exporter/","title":"Module: Exporter","text":""},{"location":"module-exporter/#promg.modules.exporter.Exporter","title":"<code>Exporter</code>","text":"<p>Create Exporter module</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.exporter import Exporter\n&gt;&gt;&gt; exporter = Exporter()\n</code></pre>"},{"location":"module-exporter/#promg.modules.exporter.Exporter.get_event_log","title":"<code>get_event_log(entity_type, additional_event_attributes=None, additional_entity_attributes=None)</code>","text":"<p>Get an event log extracted from the EKG for a specific entity and return it</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <p>The type of the entity --&gt; contract states that it exists</p> required <code>additional_event_attributes</code> <code>Optional[List[str]]</code> <p>list of different attributes of event that should also be stored in the</p> <code>None</code> <code>additional_entity_attributes</code> <code>Optional[List[str]]</code> <p>list of different attributes of entity that should also be stored in the</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of events with its attributes in the form of a dictionary</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>when the entity has not been defined</p>"},{"location":"module-exporter/#promg.modules.exporter.Exporter.save_event_log","title":"<code>save_event_log(entity_type, additional_event_attributes=None, additional_entity_attributes=None)</code>","text":"<p>Create an event log extracted from the EKG from a specific entity and store it as a csv file</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of the entity</p> required <code>additional_event_attributes</code> <code>Optional[List[str]]</code> <p>list of different attributes of event that should also be stored in the event log</p> <code>None</code> <code>additional_entity_attributes</code> <code>Optional[List[str]]</code> <p>list of different attributes of entity that should also be stored in the</p> <code>None</code>"},{"location":"module-inference_engine/","title":"Module: Inference Engine","text":""},{"location":"module-inference_engine/#promg.modules.inference_engine.InferenceEngine","title":"<code>InferenceEngine</code>","text":"<p>Create Inference Engine module</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.inference_engine import InferenceEngine\n&gt;&gt;&gt; inference_engine = InferenceEngine()\n</code></pre>"},{"location":"module-inference_engine/#promg.modules.inference_engine.InferenceEngine.match_entity_with_batch_position","title":"<code>match_entity_with_batch_position(entity_type, relative_position_type)</code>","text":"<p>Infer the batch position of a specific entity</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of the entity</p> required <code>relative_position_type</code> <code>str</code> <p>The type of the relative position</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when the entity has not been defined</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inference_engine.match_entity_with_batch_position(\n&gt;&gt;&gt;     entity_type=\"Box\",\n&gt;&gt;&gt;     relative_position_type=\"BatchPosition\")\nInfers the missing AT_POS relation between\n(:Box) and (:BatchPosition) nodes using rule shown below\n</code></pre> <p></p>"},{"location":"module-inference_engine/#promg.modules.inference_engine.InferenceEngine.infer_items_propagate_downwards_one_level","title":"<code>infer_items_propagate_downwards_one_level(entity_type)</code>","text":"<p>Infer items while propagating downwards one level</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of the entity</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when the entity has not been defined</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inference_engine.infer_items_propagate_downwards_one_level(entity_type=\"Box\")\nInfers the missing corr relation for the (:Box)\nentity using the rule shown below\n</code></pre> <p></p>"},{"location":"module-inference_engine/#promg.modules.inference_engine.InferenceEngine.infer_items_propagate_upwards_multiple_levels","title":"<code>infer_items_propagate_upwards_multiple_levels(entity_type, is_load=True)</code>","text":"<p>Infer items while propagating upwards multiple levels</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of the entity</p> required <code>is_load</code> <p>indicating whether we are inferring upwards to load events (true) or unload events (false)</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>when the entity has not been defined</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inference_engine.infer_items_propagate_upwards_multiple_levels(entity_type=\"Box\")\nInfers the missing corr relation for the (:Box)\nentity using the rule shown below\n</code></pre> <p></p>"},{"location":"module-inference_engine/#promg.modules.inference_engine.InferenceEngine.infer_items_propagate_downwards_multiple_level_w_batching","title":"<code>infer_items_propagate_downwards_multiple_level_w_batching(entity_type, relative_position_type)</code>","text":"<p>Infer items while propagating downwards multiple levels with batching</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of the entity</p> required <code>relative_position_type</code> <code>str</code> <p>The type of the relative position</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when the entity has not been defined in semantic header</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; inference_engine.infer_items_propagate_downwards_multiple_level_w_batching(\n&gt;&gt;&gt;    entity_type=\"Box\")\nInfers the missing corr relation for the (:Box)\nentity using the rule shown below\n</code></pre> <p></p>"},{"location":"module-process_discovery/","title":"Analysis Module: Process Discovery","text":""},{"location":"module-process_discovery/#promg.modules.process_discovery.ProcessDiscovery","title":"<code>ProcessDiscovery</code>","text":"<p>Create ProcessDiscovery module</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.process_discovery import ProcessDiscovery\n&gt;&gt;&gt; process_discovery = ProcessDiscovery()\n</code></pre>"},{"location":"module-process_discovery/#promg.modules.process_discovery.ProcessDiscovery.create_df_process_model","title":"<code>create_df_process_model(entity_type)</code>","text":"<p>Create a DF process model</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>The type of the entity</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when the entity has not been defined</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; process_discovery.create_df_process_model(entity_type=\"Book\")\nA process model is created from the (:Book) perspective\n</code></pre>"},{"location":"module-task_identification/","title":"Module: Task Identification","text":""},{"location":"module-task_identification/#promg.modules.task_identification.TaskIdentification","title":"<code>TaskIdentification</code>","text":"<p>Create TaskIdentification module</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.task_identification import TaskIdentification\n&gt;&gt;&gt; task_identifier = TaskIdentification(resource=\"Resource\", case=\"CASE_AWO\")\nreturns a task_identifier module from the perspective \"Resource\" and the \"CASE_AWO\" entities\n</code></pre>"},{"location":"module-task_identification/#promg.modules.task_identification.TaskIdentification.identify_tasks","title":"<code>identify_tasks()</code>","text":"<p>Method to create (:TaskInstance) nodes and [:CONTAINS] from (:Event) nodes to (:TaskInstance) nodes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.task_identification import TaskIdentification\n&gt;&gt;&gt; task_identifier = TaskIdentification(resource=\"Resource\", case=\"CASE_AWO\")\n&gt;&gt;&gt; task_identifier.identify_tasks()\nIdentifies and creates (:TaskInstance) nodes for the given resource and case\n</code></pre>"},{"location":"module-task_identification/#promg.modules.task_identification.TaskIdentification.aggregate_on_task_variant","title":"<code>aggregate_on_task_variant()</code>","text":"<p>Method to aggregate (:TaskInstance) nodes into (:TaskAggregation) nodes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.task_identification import TaskIdentification\n&gt;&gt;&gt; task_identifier = TaskIdentification(resource=\"Resource\", case=\"CASE_AWO\")\n&gt;&gt;&gt; task_identifier.aggregate_on_task_variant()\nIdentifies and creates (:TaskAggegration) given there exists (:TaskInstance) nodes\n</code></pre>"},{"location":"module-transformer/","title":"Module: Transformer","text":""},{"location":"module-transformer/#promg.modules.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>A module to transform records in the record layer/subgraphs of the semantic layer into a semantic layer using the semantic header</p> <ul> <li>Nodes on the semantic layer are created based on records or a subgraph of the semantic layer</li> <li>Relationships on the semantic layer are created based on records or a subgraph of the semantic layer</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from promg.modules.transformer import Transformer\n&gt;&gt;&gt; from promg import SemanticHeader\n&gt;&gt;&gt; semantic_header_path = Path(f'json_files/{dataset_name}.json')\n&gt;&gt;&gt; semantic_header = SemanticHeader.create_semantic_header(semantic_header_path)\n&gt;&gt;&gt; transformer = Transformer()\nReturns the transformer module\n</code></pre>"},{"location":"module-transformer/#promg.modules.transformer.Transformer.create_nodes_by_records","title":"<code>create_nodes_by_records(node_types)</code>","text":"<p>Create nodes with node types using records. If no node types are defined, then all nodes types specified in the semantic header get created if they are created using records</p> <p>Parameters:</p> Name Type Description Default <code>node_types</code> <code>Optional[List[str]]</code> <p>An optional list of strings with the node types to be created         If the given node type is not constructed using a record, then it is ignored.</p> required <p>Examples:</p> <p>Create nodes for a specific list of node types</p> <pre><code>&gt;&gt;&gt; transformer.create_nodes_by_records([\"Event\", \"Activity\", \"Book\"])\nThe transformer creates the nodes with the types Event, Activity and Book\nas specified in semantic header in the Event Knowledge graph.\n</code></pre> <p>Create nodes for all node types specified in the semantic header</p> <pre><code>&gt;&gt;&gt; transformer.create_nodes_by_records()\nThe transformer creates the nodes that are constructed by a record\nas specified in the semantic header Event Knowledge graph.\n</code></pre>"},{"location":"module-transformer/#promg.modules.transformer.Transformer.create_relations_using_records","title":"<code>create_relations_using_records(relation_types)</code>","text":"<p>Create relationships with relation types using records. That is two nodes related to the same (:Record) node get a relation in between as specified in the semantic header. If no relation types are defined, then all relation types specified in the semantic header get created if they are created using records</p> <p>Parameters:</p> Name Type Description Default <code>relation_types</code> <code>Optional[List[str]]</code> <p>An optional list of strings with the relation types to be created             If the given relation type is not constructed using a record, then it is ignored.</p> required <p>Examples:</p> <p>Create nodes for a specific list of relation types</p> <pre><code>&gt;&gt;&gt; transformer.create_relations_using_records([\"MEMBER_OF\"])\nThe transformer creates the relationships between two nodes with the type MEMBER_OF\nas specified in semantic header in the Event Knowledge graph.\nMore specifically, a (:Member) node is related to a (:Library) node with a [:MEMBER_OF] relation\nwhen the (:Member) and (:Library) node are related to the same (:Record) node as specified in\nthe semantic header\n</code></pre> <p>Create nodes for all relationship types specified in the semantic header</p> <pre><code>&gt;&gt;&gt; transformer.create_relations_using_records()\nThe transformer creates the relationships that are constructed by a subgraph\nas specified in the semantic header Event Knowledge graph.\n</code></pre>"},{"location":"module-transformer/#promg.modules.transformer.Transformer.create_relations_using_relations","title":"<code>create_relations_using_relations(relation_types)</code>","text":"<p>Create relations with relation types using subgraph of semantic layer (relations). If no relation types are defined, then all relation types specified in the semantic header get created if they are created using a subgraph</p> <p>Parameters:</p> Name Type Description Default <code>relation_types</code> <code>Optional[List[str]]</code> <p>An optional list of strings with the relation types to be created.             If the given relation type is not constructed using a subgraph, then it is ignored.</p> required <p>Examples:</p> <p>Create nodes for a specific list of node types</p> <pre><code>&gt;&gt;&gt; transformer.create_relations_using_relations([\"BORROWED\"])\nThe transformer creates the relationship with the types BORROWED\nas specified in semantic header in the Event Knowledge graph.\n</code></pre> <p>Create nodes for all node types specified in the semantic header</p> <pre><code>&gt;&gt;&gt; transformer.create_relations_using_relations()\nThe transformer creates the relationship that are constructed by a subgraph\nas specified in the semantic header Event Knowledge graph.\n</code></pre>"},{"location":"module-transformer/#promg.modules.transformer.Transformer.create_df_edges","title":"<code>create_df_edges(entity_types=None, event_label='Event')</code>","text":"<p>For all nodes (nodes or reified relationship nodes) with entity_types create DF edges between (:Event) nodes matching event_label</p> <p>Parameters:</p> Name Type Description Default <code>entity_types</code> <code>List[str]</code> <p>Optional list of types for which we want to create the DF edges.           If none, then we create DF edges for all nodes as specified in the semantic header.           If for the given entity types the infer_df is False (or not specified) in the semantic header,           then it is ignored (i.e. no DF edges are created)</p> <code>None</code> <code>event_label</code> <code>str</code> <p>The label of the event nodes used to create the DF edges</p> <code>'Event'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; self.create_df_edges(entity_types=[\"Book\"], event_label=\"Event\")\n</code></pre>"},{"location":"module-transformer/#promg.modules.transformer.Transformer.merge_duplicate_df","title":"<code>merge_duplicate_df()</code>","text":"<p>Merge duplicate DF edges between the same nodes Only do this for the (reified) nodes in semantic header with merge_duplicate_df = true</p>"},{"location":"related_publications/","title":"Related publications","text":"<p>The following projects are part of this repository</p>"},{"location":"related_publications/#event-knowledge-graphs","title":"Event Knowledge Graphs","text":"<p>We use Event Knowledge Graphs as basis for our data model as they already naturally model Events, Activities, Entities and their relations for Process Mining. The EKGs are stored in a labeled property graph in Neo4J.</p>"},{"location":"related_publications/#publications","title":"Publications","text":"<ul> <li><code>Stefan Esser, Dirk Fahland: Multi-Dimensional Event Data in Graph   Databases. [CoRR abs/2005.14552](https://arxiv.org/abs/2005.14552), [Journal on Data Semantics, DOI: 10.1007/s13740-021-00122-1](https://dx.doi.org/10.1007/s13740-021-00122-1) (   2020)</code></li> <li><code>Esser, Stefan. (2020, February 19). A Schema Framework for Graph Event Data. Master thesis. Eindhoven University of   Technology. https://doi.org/10.5281/zenodo.3820037</code></li> </ul>"},{"location":"related_publications/#oced-pg","title":"OCED-PG","text":"<p>We developed a reference implementation for Object-Centric Event Data OCED. OCED-PG is a declarative extract-load-transformT framework, that maps the raw data to a corresponding EKG, using the semantic header as a basis.</p> <p>We proposed a three-layer approach to create a semantic-aware representation and storage system for OCED. </p> <ul> <li> <p>Base ontology: The OCED proposal is formalized as a PG-schema [1] providing a common interface for process querying. The schema defines a base ontology for representing and transforming OCED, which includes a semantic layer (defining the OCED concepts) and a record layer (defining concepts for generic data records from a legacy system and how they are related to the semantic layer). </p> </li> <li> <p>Reference ontology: The base ontology is specialized into a domain-specific reference ontology using PG-schema's inheritance mechanism. The reference ontology includes a semantic layer (defining the domain's semantic objects, events, and relations), and a record layer (defining in which legacy records the domain-level concepts are stored). The structural definitions are extended with rules to transform data in the record layer into nodes and relationships of the semantic layer, similar to ontology-based data access.</p> </li> <li>OCED-PG: declarative extract-load-transform (ELT) framework. OCED-PG load the legacy data records into the graph DB as a record layer. We then transform the data records into OCED by automatically translating the transformation rules of step (2) into queries over the record layer.</li> </ul>"},{"location":"related_publications/#multi-process-discovery-and-analysis","title":"Multi-process Discovery and Analysis","text":"<p><code>D. Fahland, \u201cProcess mining over multiple behavioral dimensions with event knowledge graphs,\u201d in Process Mining Handbook. Springer, 2022, vol. 448, pp. 274\u2013319</code></p>"},{"location":"related_publications/#task-identification","title":"Task Identification","text":"<p><code>Klijn, E.L., Mannhardt, F., Fahland, D. (2021). Classifying and Detecting Task Executions and Routines in Processes Using Event Graphs. In: Polyvyanyy, A., Wynn, M.T., Van Looy, A., Reichert, M. (eds) Business Process Management Forum. BPM 2021. Lecture Notes in Business Information Processing, vol 427. Springer, Cham. https://doi.org/10.1007/978-3-030-85440-9_13</code></p>"},{"location":"related_publications/#inference-of-missing-case-identifiers","title":"Inference of missing case identifiers","text":"<p><code>Swevels, A., Dijkman, R., Fahland, D. (2023). Inferring Missing Entity Identifiers from Context Using Event Knowledge Graphs. In: Di Francescomarino, C., Burattin, A., Janiesch, C., Sadiq, S. (eds) Business Process Management. BPM 2023. Lecture Notes in Computer Science, vol 14159. Springer, Cham. https://doi.org/10.1007/978-3-031-41620-0_11</code></p>"},{"location":"tutorials/","title":"Tutorial for BPIC'17 Dataset","text":""},{"location":"tutorials/#download-datasets-and-place-in-correct-directory","title":"Download Datasets and place in correct directory","text":"<p>We use the BPIC'17 dataset, dataset descriptions and semantic header available from</p> <p><code>Swevels, Ava, &amp; Fahland, Dirk. (2023). Event Data and Semantic Header for OCED-PG [Data set]. Zenodo. https://doi.org/10.5281/zenodo.8296559</code></p> <p>We assume the dataset to be present in <code>/data/BPIC17</code>. If you want to change the directory, you have to change the directory in the dataset description (<code>BPIC17_DS.json</code>) as well.</p> <p>The semantic header (<code>BPIC17.json</code>) and the dataset description (<code>BPIC17_DS.json</code>) to be present in <code>[json_directory]</code>.</p>"},{"location":"tutorials/#install-promg","title":"Install PromG","text":"<p>You can install PromG using <code>pip install promg</code>.</p>"},{"location":"tutorials/#set-up-variables","title":"Set up variables","text":"<p>We first set some common variables used in the main script.</p> Set up parameters<pre><code># import required packages\nimport os\nfrom pathlib import Path\ndataset_name = 'BPIC17' # name of the dataset\n# use a sample when importing the data, useful for testing\nuse_sample = False \n# the size for our batch queries, recommended from 10000 to 100000.\nbatch_size = 100000\n# use preprocessed files, saves times (skip preprocessing)\n# if these files don't exist, they will be created\nuse_preprocessed_files = False \n# set the paths for the semantic header and the dataset description\nsemantic_header_path = Path(f'[json_directory]/{dataset_name}.json')\nds_path = Path(f'json_files/{dataset_name}_DS.json')\n</code></pre>"},{"location":"tutorials/#determine-settings","title":"Determine settings","text":"<p>Depending on your analysis, you want to execute some steps.  We control these steps using booleans.</p> Determine settings<pre><code># import required packages\nstep_clear_db = True\nstep_populate_graph = True\nstep_discover_model = True\nstep_build_tasks = True\nstep_infer_delays = True\nverbose = False\n</code></pre>"},{"location":"tutorials/#set-credentials","title":"Set Credentials","text":"<p>If you want to use the default credentials, you can use the following piece of code.</p> Set Default Credentials<pre><code>from promg import authentication\n# set the credentials key to default\ncredentials_key = authentication.Connections.LOCAL\n</code></pre> <p>If you want to have custom credentials (i.e. remote), you can use the following piece of code. Set Custom Credentials<pre><code>from promg.database_managers.authentication import Credentials\n# if you want a remote/other connection, \n# store credentials somewhere different (e.g. remote_authentication)\n# custom credentials\ncustom = Credentials(\nuri=\"your_uri\", #e.g. uri=\"bolt://localhost:7687\",\nuser=\"your_user\", #e.g. user=\"neo4j\",\npassword=\"your_password\" #e.g. password=\"12345678\"\n)\n# if you have created your own credentials\ncredentials_key = custom\n</code></pre></p>"},{"location":"tutorials/#create-semantic-header-and-dataset-description-objects","title":"Create Semantic header and dataset description objects","text":"Create Semantic header and dataset description objects<pre><code>from promg import SemanticHeader, DatasetDescriptions\n# create semantic header object\nsemantic_header = SemanticHeader.create_semantic_header(semantic_header_path)\n# create dataset description object\ndataset_descriptions = DatasetDescriptions(ds_path)\n</code></pre>"},{"location":"tutorials/#set-up-database-connection","title":"Set up database connection","text":"Create DatabaseConnection<pre><code>from promg import DatabaseConnection\ndb_connection = DatabaseConnection.set_up_connection_using_key(key=credentials_key,\nverbose=verbose)\n</code></pre>"},{"location":"tutorials/#set-up-performance-tracker","title":"Set up Performance Tracker","text":"<p>The performance tracker let's you know the progress of the steps and also outputs a performance log Stored in <code>\\perf\\</code>.</p> Create Performance<pre><code>from promg import Performance\nperformance = Performance.set_up_performance(dataset_name=dataset_name,\nuse_sample=use_sample)\n</code></pre>"},{"location":"tutorials/#clear-database","title":"Clear Database","text":"<p>Sometimes you want to clear the entire database, you can do so using the following piece of code.</p> Clear Database<pre><code>from promg.modules.db_management import DBManagement\ndb_manager = DBManagement()\nif step_clear_db:\nprint(Fore.RED + 'Clearing the database.' + Fore.RESET)\ndb_manager.clear_db(replace=True)\ndb_manager.set_constraints()\n</code></pre>"},{"location":"tutorials/#load-and-transform-the-database-using-ocedpg","title":"Load and Transform the database using OcedPG","text":"Populate Graph using OcedPG<pre><code>    from promg import OcedPg\nif step_populate_graph:\nif use_preprocessed_files:\nprint(Fore.RED + '\ud83d\udcbe Preloaded files are used!' + Fore.RESET)\nelse:\nprint(Fore.RED + '\ud83d\udcdd Importing and creating files' + Fore.RESET)\noced_pg = OcedPg(dataset_descriptions=dataset_descriptions,\nuse_sample=use_sample,\nuse_preprocessed_files=use_preprocessed_files)\noced_pg.load_and_transform()\noced_pg.create_df_edges()\n</code></pre>"},{"location":"tutorials/#identify-tasks","title":"Identify Tasks","text":"Identify Tasks<pre><code>    from promg.modules.task_identification import TaskIdentification\nif step_build_tasks:\nprint(Fore.RED + 'Detecting tasks.' + Fore.RESET)\ntask_identifier = TaskIdentification(resource=\"Resource\", case=\"CaseAWO\")\ntask_identifier.identify_tasks()\ntask_identifier.aggregate_on_task_variant()\n</code></pre>"},{"location":"tutorials/#custom-modules","title":"Custom modules","text":"<p>To be documented.</p>"},{"location":"tutorials/#close-database-connection-and-finish-performance","title":"Close database connection and finish performance","text":"finish code<pre><code>    performance.finish_and_save()\ndb_manager.print_statistics()\ndb_connection.close_connection()\n</code></pre>"},{"location":"tutorials/#complete-code","title":"Complete code","text":"Complete Code<pre><code>import os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom promg import SemanticHeader, OcedPg\nfrom promg import DatabaseConnection\nfrom promg import authentication\nfrom promg import DatasetDescriptions\nfrom promg import Performance\nfrom promg.modules.db_management import DBManagement\nfrom promg.modules.task_identification import TaskIdentification\n# several steps of import, each can be switch on/off\nfrom colorama import Fore\ndataset_name = 'BPIC17'\nuse_sample = False\nbatch_size = 10000\nuse_preprocessed_files = False\nsemantic_header_path = Path(f'json_files/{dataset_name}.json')\nsemantic_header = SemanticHeader.create_semantic_header(semantic_header_path)\nds_path = Path(f'json_files/{dataset_name}_DS.json')\ndataset_descriptions = DatasetDescriptions(ds_path)\nstep_clear_db = False\nstep_populate_graph = False\nstep_delete_parallel_df = False\nstep_discover_model = True\nstep_build_tasks = False\nstep_infer_delays = True\nverbose = False\ncredentials_key = authentication.Connections.LOCAL\ndef main() -&gt; None:\n\"\"\"\n    Main function, read all the logs, clear and create the graph, perform checks\n    @return: None\n    \"\"\"\nprint(\"Started at =\", datetime.now().strftime(\"%H:%M:%S\"))\ndb_connection = DatabaseConnection.set_up_connection_using_key(key=credentials_key,\nverbose=verbose)\nperformance = Performance.set_up_performance(dataset_name=dataset_name,\nuse_sample=use_sample)\ndb_manager = DBManagement()\nif step_clear_db:\nprint(Fore.RED + 'Clearing the database.' + Fore.RESET)\ndb_manager.clear_db(replace=True)\ndb_manager.set_constraints()\nif step_populate_graph:\nif use_preprocessed_files:\nprint(Fore.RED + '\ud83d\udcbe Preloaded files are used!' + Fore.RESET)\nelse:\nprint(Fore.RED + '\ud83d\udcdd Importing and creating files' + Fore.RESET)\noced_pg = OcedPg(dataset_descriptions=dataset_descriptions,\nuse_sample=use_sample,\nuse_preprocessed_files=use_preprocessed_files)\noced_pg.load_and_transform()\noced_pg.create_df_edges()\nif dataset_name == 'BPIC17':\nif step_build_tasks:\nprint(Fore.RED + 'Detecting tasks.' + Fore.RESET)\ntask_identifier = TaskIdentification(resource=\"Resource\", case=\"CaseAWO\")\ntask_identifier.identify_tasks()\ntask_identifier.aggregate_on_task_variant()\nperformance.finish_and_save()\ndb_manager.print_statistics()\ndb_connection.close_connection()\nif __name__ == \"__main__\":\nmain()\n</code></pre>"}]}